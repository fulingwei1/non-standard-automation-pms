# GLM (æ™ºè°±AI) API é…ç½®è¯´æ˜

**é…ç½®å®Œæˆæ—¶é—´**: 2026-02-15 22:23  
**é…ç½®äºº**: ç¬¦å“¥

---

## âœ… é…ç½®å®Œæˆ

### 1. é…ç½®æ–‡ä»¶æ›´æ–°

å·²åœ¨ä»¥ä¸‹æ–‡ä»¶ä¸­æ·»åŠ GLMé…ç½®ï¼š

#### `app/core/config.py`
```python
# GLM (æ™ºè°±AI) é…ç½®
GLM_API_KEY: Optional[str] = None  # GLM API Key
GLM_API_BASE: str = "https://open.bigmodel.cn/api/paas/v4"  # GLM API åŸºç¡€URL
GLM_MODEL: str = "glm-4"  # é»˜è®¤æ¨¡å‹
GLM_MAX_TOKENS: int = 4000  # æœ€å¤§ç”Ÿæˆtokenæ•°
GLM_TEMPERATURE: float = 0.7  # æ¸©åº¦å‚æ•°
GLM_TIMEOUT: int = 30  # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
GLM_ENABLED: bool = False  # æ˜¯å¦å¯ç”¨GLM AIåŠŸèƒ½
```

#### `.env` (å·²æ·»åŠ ä½ çš„API Key)
```bash
# GLM (æ™ºè°±AI) é…ç½®
GLM_API_KEY=8677faa1d4a54f4bb7d171069e9d84f9.TSMGwqPbEyTx3pja
GLM_ENABLED=true
GLM_MODEL=glm-4
GLM_API_BASE=https://open.bigmodel.cn/api/paas/v4
```

---

## ğŸ“š ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•ä¸€ï¼šåœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from app.core.config import settings
import requests

def call_glm_api(prompt: str):
    """è°ƒç”¨GLM API"""
    if not settings.GLM_ENABLED:
        raise ValueError("GLMæœªå¯ç”¨")
    
    headers = {
        "Authorization": f"Bearer {settings.GLM_API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": settings.GLM_MODEL,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": settings.GLM_MAX_TOKENS,
        "temperature": settings.GLM_TEMPERATURE
    }
    
    response = requests.post(
        f"{settings.GLM_API_BASE}/chat/completions",
        headers=headers,
        json=data,
        timeout=settings.GLM_TIMEOUT
    )
    
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
result = call_glm_api("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹æ™ºè°±AI")
print(result['choices'][0]['message']['content'])
```

### æ–¹æ³•äºŒï¼šåˆ›å»ºGLM Service

```python
# app/services/glm_service.py
from typing import Optional
from app.core.config import settings
import requests
import logging

logger = logging.getLogger(__name__)


class GLMService:
    """æ™ºè°±AI GLMæœåŠ¡"""
    
    def __init__(self):
        self.api_key = settings.GLM_API_KEY
        self.api_base = settings.GLM_API_BASE
        self.model = settings.GLM_MODEL
        self.enabled = settings.GLM_ENABLED
    
    def is_enabled(self) -> bool:
        """æ£€æŸ¥GLMæ˜¯å¦å¯ç”¨"""
        return self.enabled and bool(self.api_key)
    
    def chat(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """
        è°ƒç”¨GLMå¯¹è¯API
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆå¯é€‰ï¼‰
            temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            GLMçš„å›å¤æ–‡æœ¬
        """
        if not self.is_enabled():
            raise ValueError("GLMæœåŠ¡æœªå¯ç”¨æˆ–æœªé…ç½®API Key")
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": self.model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens or settings.GLM_MAX_TOKENS,
            "temperature": temperature or settings.GLM_TEMPERATURE
        }
        
        try:
            response = requests.post(
                f"{self.api_base}/chat/completions",
                headers=headers,
                json=data,
                timeout=settings.GLM_TIMEOUT
            )
            response.raise_for_status()
            
            result = response.json()
            return result['choices'][0]['message']['content']
            
        except Exception as e:
            logger.error(f"GLM APIè°ƒç”¨å¤±è´¥: {e}")
            raise


# ä½¿ç”¨ç¤ºä¾‹
glm = GLMService()
if glm.is_enabled():
    reply = glm.chat("å¸®æˆ‘ç”Ÿæˆä¸€ä»½é¡¹ç›®éœ€æ±‚åˆ†ææŠ¥å‘Š")
    print(reply)
```

---

## ğŸ¯ åœ¨PMSç³»ç»Ÿä¸­é›†æˆGLM

### åœºæ™¯1ï¼šAIè¾…åŠ©æŠ¥ä»·å•ç”Ÿæˆ

```python
# app/api/v1/endpoints/sales/quotes.py
from app.services.glm_service import GLMService

@router.post("/quotes/ai-generate")
def generate_quote_with_ai(
    customer_name: str,
    product_type: str,
    requirements: str,
    db: Session = Depends(get_db)
):
    """AIè¾…åŠ©ç”ŸæˆæŠ¥ä»·å•"""
    glm = GLMService()
    
    prompt = f"""
    è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„è‡ªåŠ¨åŒ–æµ‹è¯•è®¾å¤‡æŠ¥ä»·å•ï¼š
    - å®¢æˆ·ï¼š{customer_name}
    - äº§å“ç±»å‹ï¼š{product_type}
    - éœ€æ±‚æè¿°ï¼š{requirements}
    
    æŠ¥ä»·å•åº”åŒ…æ‹¬ï¼šè®¾å¤‡é…ç½®ã€æŠ€æœ¯å‚æ•°ã€æŠ¥ä»·æ˜ç»†ã€äº¤æœŸè¯´æ˜
    """
    
    ai_quote = glm.chat(prompt)
    
    return {
        "ai_generated_quote": ai_quote,
        "customer": customer_name,
        "product_type": product_type
    }
```

### åœºæ™¯2ï¼šAIè¾…åŠ©æŠ€æœ¯æ–‡æ¡£æ’°å†™

```python
# app/api/v1/endpoints/engineer_performance/knowledge.py
from app.services.glm_service import GLMService

@router.post("/knowledge/ai-improve")
def improve_document_with_ai(
    document_id: int,
    original_content: str,
    db: Session = Depends(get_db)
):
    """AIè¾…åŠ©æ”¹è¿›æŠ€æœ¯æ–‡æ¡£"""
    glm = GLMService()
    
    prompt = f"""
    è¯·å¸®æˆ‘æ”¹è¿›è¿™ä»½æŠ€æœ¯æ–‡æ¡£ï¼Œä½¿å…¶æ›´ä¸“ä¸šã€æ¸…æ™°ï¼š
    
    åŸæ–‡ï¼š
    {original_content}
    
    è¯·ä»ä»¥ä¸‹æ–¹é¢æ”¹è¿›ï¼š
    1. é€»è¾‘ç»“æ„ä¼˜åŒ–
    2. ä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§
    3. å¯è¯»æ€§æå‡
    4. è¡¥å……å¿…è¦çš„æŠ€æœ¯ç»†èŠ‚
    """
    
    improved_content = glm.chat(prompt, max_tokens=8000)
    
    return {
        "original": original_content,
        "improved": improved_content
    }
```

### åœºæ™¯3ï¼šAIæ™ºèƒ½é—®ç­”ï¼ˆé¡¹ç›®ç®¡ç†åŠ©æ‰‹ï¼‰

```python
# app/api/v1/endpoints/ai_assistant.py
from app.services.glm_service import GLMService

@router.post("/ai/ask")
def ai_assistant(
    question: str,
    context: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """AIé¡¹ç›®ç®¡ç†åŠ©æ‰‹"""
    glm = GLMService()
    
    # æ„å»ºä¸Šä¸‹æ–‡æç¤º
    system_context = """
    ä½ æ˜¯é‡‘å‡¯åšè‡ªåŠ¨åŒ–æµ‹è¯•è®¾å¤‡å…¬å¸çš„é¡¹ç›®ç®¡ç†åŠ©æ‰‹ã€‚
    ä½ ç†Ÿæ‚‰ICTã€FCTã€AOIç­‰æµ‹è¯•è®¾å¤‡çš„é¡¹ç›®ç®¡ç†æµç¨‹ã€‚
    è¯·åŸºäºä¸“ä¸šçŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    """
    
    full_prompt = f"{system_context}\n\nç”¨æˆ·é—®é¢˜ï¼š{question}"
    if context:
        full_prompt += f"\n\nç›¸å…³ä¸Šä¸‹æ–‡ï¼š{context}"
    
    answer = glm.chat(full_prompt)
    
    return {
        "question": question,
        "answer": answer
    }
```

---

## ğŸ”§ é…ç½®å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | å¯é€‰å€¼ |
|------|------|--------|--------|
| `GLM_API_KEY` | APIå¯†é’¥ | None | ä½ çš„API Key |
| `GLM_ENABLED` | æ˜¯å¦å¯ç”¨ | False | true/false |
| `GLM_MODEL` | æ¨¡å‹åç§° | glm-4 | glm-4, glm-4v, glm-3-turbo |
| `GLM_API_BASE` | APIåŸºç¡€URL | https://open.bigmodel.cn/api/paas/v4 | - |
| `GLM_MAX_TOKENS` | æœ€å¤§tokenæ•° | 4000 | 100-8000 |
| `GLM_TEMPERATURE` | æ¸©åº¦å‚æ•° | 0.7 | 0.0-1.0 |
| `GLM_TIMEOUT` | è¶…æ—¶æ—¶é—´(ç§’) | 30 | 10-60 |

---

## ğŸ” å®‰å…¨å»ºè®®

1. **ä¸è¦æäº¤API Keyåˆ°Git**
   - `.env` æ–‡ä»¶å·²åœ¨ `.gitignore` ä¸­
   - ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–å¯†é’¥ç®¡ç†æœåŠ¡

2. **API Keyè½®è½¬**
   - å®šæœŸæ›´æ¢API Key
   - æ™ºè°±AIæ§åˆ¶å°å¯ä»¥ç”Ÿæˆæ–°Key

3. **è°ƒç”¨é¢‘ç‡æ§åˆ¶**
   - å»ºè®®æ·»åŠ ç¼“å­˜æœºåˆ¶
   - é¿å…åŒä¸€é—®é¢˜é‡å¤è°ƒç”¨

---

## ğŸ“Š è´¹ç”¨è¯´æ˜

æ™ºè°±AI GLM-4 è®¡è´¹æ–¹å¼ï¼ˆä»¥å®˜ç½‘ä¸ºå‡†ï¼‰ï¼š
- **GLM-4**: çº¦ Â¥0.1/åƒtokensï¼ˆè¾“å…¥ï¼‰+ Â¥0.1/åƒtokensï¼ˆè¾“å‡ºï¼‰
- **GLM-3-Turbo**: çº¦ Â¥0.005/åƒtokensï¼ˆæ›´ä¾¿å®œï¼‰

**æˆæœ¬æ§åˆ¶å»ºè®®**ï¼š
- å¼€å‘/æµ‹è¯•ç¯å¢ƒä½¿ç”¨ GLM-3-Turbo
- ç”Ÿäº§ç¯å¢ƒæ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©æ¨¡å‹
- è®¾ç½® `GLM_MAX_TOKENS` é™åˆ¶å•æ¬¡æ¶ˆè€—

---

## ğŸ§ª æµ‹è¯•é…ç½®

### å¿«é€Ÿæµ‹è¯•

```bash
# æµ‹è¯•APIè¿é€šæ€§
curl -X POST "https://open.bigmodel.cn/api/paas/v4/chat/completions" \
  -H "Authorization: Bearer 8677faa1d4a54f4bb7d171069e9d84f9.TSMGwqPbEyTx3pja" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "glm-4",
    "messages": [
      {"role": "user", "content": "ä½ å¥½"}
    ]
  }'
```

### Pythonæµ‹è¯•è„šæœ¬

```python
# test_glm.py
import requests
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("GLM_API_KEY")
api_base = os.getenv("GLM_API_BASE")

response = requests.post(
    f"{api_base}/chat/completions",
    headers={
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    },
    json={
        "model": "glm-4",
        "messages": [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä½ è‡ªå·±"}
        ]
    }
)

print(response.json())
```

---

## ğŸ” å¸¸è§é—®é¢˜

### Q: API Keyå¤±æ•ˆæ€ä¹ˆåŠï¼Ÿ
A: ç™»å½•æ™ºè°±AIæ§åˆ¶å°é‡æ–°ç”Ÿæˆï¼Œç„¶åæ›´æ–° `.env` æ–‡ä»¶

### Q: å¦‚ä½•åˆ‡æ¢æ¨¡å‹ï¼Ÿ
A: ä¿®æ”¹ `.env` ä¸­çš„ `GLM_MODEL` å‚æ•°ï¼š
```bash
GLM_MODEL=glm-3-turbo  # æ›´å¿«æ›´ä¾¿å®œ
# æˆ–
GLM_MODEL=glm-4v  # æ”¯æŒè§†è§‰ç†è§£
```

### Q: è°ƒç”¨å¤±è´¥å¦‚ä½•æ’æŸ¥ï¼Ÿ
A: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. API Keyæ˜¯å¦æ­£ç¡®
2. ç½‘ç»œæ˜¯å¦é€šç•…
3. ä½™é¢æ˜¯å¦å……è¶³
4. è¯·æ±‚æ ¼å¼æ˜¯å¦æ­£ç¡®

---

## ğŸ“– å‚è€ƒæ–‡æ¡£

- [æ™ºè°±AIå®˜æ–¹æ–‡æ¡£](https://open.bigmodel.cn/dev/api)
- [GLM-4 æ¨¡å‹ä»‹ç»](https://open.bigmodel.cn/dev/howuse/model)
- [APIæ¥å£è¯´æ˜](https://open.bigmodel.cn/dev/api#overview)

---

**é…ç½®å®Œæˆï¼** âœ…

ç¬¦å“¥ï¼Œä½ çš„GLM API Keyå·²æˆåŠŸé…ç½®åˆ°ç³»ç»Ÿä¸­ã€‚å¯ä»¥å¼€å§‹åœ¨ä»£ç ä¸­ä½¿ç”¨äº†ï¼

å¦‚éœ€å¸®åŠ©é›†æˆåˆ°å…·ä½“åŠŸèƒ½ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸ’ªğŸ¾

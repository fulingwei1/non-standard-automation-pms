# 生产进度模块 - 管理员手册

## 1. 系统配置

### 1.1 基础配置

#### 1.1.1 车间配置
**路径**: 系统管理 > 车间管理

```sql
-- 车间表结构
CREATE TABLE workshop (
    id INT PRIMARY KEY,
    name VARCHAR(50),
    code VARCHAR(20),
    capacity_hours INT,  -- 日产能(小时)
    status VARCHAR(20)
);
```

**配置项**:
- 车间名称和编码
- 日产能(工时)
- 工作班次
- 负责人

#### 1.1.2 工位配置
**配置要点**:
- 工位编号规则: `[车间代码]-[区域]-[序号]`
- 绑定设备
- 设置标准产能
- 配置技能要求

#### 1.1.3 工序字典
```
工序配置模板:
├─ 工序编码: OP-001
├─ 工序名称: 焊接
├─ 标准工时: 0.08h/件
├─ 质量标准: QS-WLD-001
├─ 技能要求: 焊接4级以上
└─ 前置工序: OP-000 (无)
```

---

### 1.2 权限配置

#### 1.2.1 角色定义
| 角色 | 权限范围 |
|------|---------|
| 生产主管 | 全部生产数据，审批报工，调整排程 |
| 排程员 | 查看/编辑排程，查看产能 |
| 质检员 | 质检录入，查看质量数据 |
| 工人 | 查看自己的工单，提交报工 |
| 系统管理员 | 系统配置，用户管理 |

#### 1.2.2 数据权限
```python
# 数据隔离规则
data_scope_rules = {
    "生产主管": "本车间全部数据",
    "排程员": "本车间排程数据",
    "质检员": "本车间质检数据", 
    "工人": "仅本人工单数据",
    "系统管理员": "全部数据"
}
```

**配置步骤**:
1. 创建角色
2. 分配权限点
3. 设置数据范围
4. 分配用户到角色

---

## 2. 参数配置

### 2.1 排程参数

```yaml
# config/scheduling.yml
scheduling:
  algorithm:
    default: GENETIC
    population_size: 100
    max_generations: 200
    crossover_rate: 0.8
    mutation_rate: 0.1
  
  constraints:
    max_overtime_hours: 2
    skill_match_required: true
    deadline_strict_mode: true
  
  optimization:
    makespan_weight: 0.4
    utilization_weight: 0.3
    tardiness_weight: 0.3
```

### 2.2 质量参数

```yaml
# config/quality.yml
quality:
  defect_rate_threshold: 0.05  # 5%
  spc:
    control_limit_sigma: 3
    trend_point_count: 7
    sample_size_min: 30
  
  alert_rules:
    - name: "不良率预警"
      type: DEFECT_RATE
      threshold: 5.0
      operator: GT
      alert_level: WARNING
      
    - name: "不良率严重"
      type: DEFECT_RATE  
      threshold: 10.0
      operator: GT
      alert_level: CRITICAL
```

### 2.3 工时参数

```yaml
# config/workhour.yml
workhour:
  validation:
    max_daily_hours: 12
    max_continuous_days: 7
    deviation_threshold: 0.3  # ±30%
  
  anomaly_detection:
    enabled: true
    rules:
      - EXCESSIVE_HOURS  # 单日>12h
      - NO_REST          # 连续7天
      - STATISTICAL_OUTLIER  # 3σ原则
```

---

## 3. 数据管理

### 3.1 数据备份

#### 3.1.1 自动备份
```bash
# /etc/cron.d/production-backup
# 每日凌晨2点全量备份
0 2 * * * /opt/backup/full_backup.sh

# 每小时增量备份
0 * * * * /opt/backup/incremental_backup.sh
```

**备份脚本**:
```bash
#!/bin/bash
# full_backup.sh

BACKUP_DIR="/backup/production"
DATE=$(date +%Y%m%d)

# 备份数据库
mysqldump -u root -p \
  --databases production_db \
  --single-transaction \
  --master-data=2 \
  | gzip > $BACKUP_DIR/db_$DATE.sql.gz

# 备份附件
tar -czf $BACKUP_DIR/attachments_$DATE.tar.gz \
  /var/www/attachments

# 删除30天前的备份
find $BACKUP_DIR -name "*.gz" -mtime +30 -delete

# 上传到OSS (可选)
aws s3 sync $BACKUP_DIR s3://backups/production/
```

#### 3.1.2 手动备份
```sql
-- 备份指定日期数据
SELECT * INTO OUTFILE '/tmp/work_orders_20260217.csv'
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
FROM work_order
WHERE DATE(created_at) = '2026-02-17';
```

---

### 3.2 数据恢复

#### 3.2.1 全量恢复
```bash
# 恢复数据库
gunzip < /backup/db_20260217.sql.gz | mysql -u root -p production_db

# 恢复附件
tar -xzf /backup/attachments_20260217.tar.gz -C /var/www/
```

#### 3.2.2 部分恢复
```sql
-- 恢复单个表
SOURCE /backup/work_order_table.sql;

-- 恢复特定记录
INSERT INTO work_order 
SELECT * FROM work_order_backup 
WHERE id IN (1001, 1002, 1003);
```

---

### 3.3 数据归档

**归档策略**:
```python
# 数据归档规则
archive_rules = {
    'work_report': {
        'retention_days': 90,
        'archive_to': 'work_report_archive',
        'compress': True
    },
    'quality_inspection': {
        'retention_days': 365,
        'archive_to': 'quality_inspection_archive',
        'compress': True
    }
}
```

**归档脚本**:
```python
#!/usr/bin/env python3
# archive_data.py

from datetime import datetime, timedelta

def archive_old_data(table_name, retention_days):
    """归档历史数据"""
    cutoff_date = datetime.now() - timedelta(days=retention_days)
    
    # 导出数据
    export_sql = f"""
    SELECT * INTO OUTFILE '/archive/{table_name}_{datetime.now():%Y%m%d}.csv'
    FROM {table_name}
    WHERE created_at < '{cutoff_date}'
    """
    
    # 压缩
    os.system(f"gzip /archive/{table_name}_*.csv")
    
    # 删除原数据
    delete_sql = f"""
    DELETE FROM {table_name}
    WHERE created_at < '{cutoff_date}'
    """
```

---

## 4. 性能优化

### 4.1 数据库优化

#### 4.1.1 索引优化
```sql
-- 查看慢查询
SELECT * FROM mysql.slow_log
WHERE query_time > 1
ORDER BY query_time DESC
LIMIT 10;

-- 添加复合索引
CREATE INDEX idx_work_order_status_date 
ON work_order(status, plan_start_date);

-- 分析索引效率
EXPLAIN SELECT * FROM work_order 
WHERE status = 'IN_PROGRESS' 
AND plan_start_date >= '2026-02-17';
```

#### 4.1.2 查询优化
```sql
-- 优化前 (慢)
SELECT * FROM work_report wr
JOIN work_order wo ON wr.work_order_id = wo.id
WHERE DATE(wr.created_at) = '2026-02-17';

-- 优化后 (快)
SELECT * FROM work_report wr
JOIN work_order wo ON wr.work_order_id = wo.id
WHERE wr.created_at >= '2026-02-17 00:00:00'
AND wr.created_at < '2026-02-18 00:00:00';
```

### 4.2 缓存优化

#### 4.2.1 Redis配置
```redis
# redis.conf 关键配置

# 最大内存
maxmemory 2gb

# 淘汰策略 (LRU)
maxmemory-policy allkeys-lru

# 持久化
save 900 1
save 300 10
save 60 10000

# AOF
appendonly yes
appendfsync everysec
```

#### 4.2.2 缓存预热
```python
# cache_warmup.py

def warmup_cache():
    """系统启动时预热缓存"""
    # 1. 加载活跃工单
    active_orders = db.query(WorkOrder).filter_by(
        status='IN_PROGRESS'
    ).all()
    
    for order in active_orders:
        cache_key = f"work_order:{order.id}"
        redis.setex(cache_key, 3600, order.to_json())
    
    # 2. 加载产能数据
    for workshop in Workshop.query.all():
        capacity = calculate_capacity(workshop.id)
        redis.setex(
            f"capacity:{workshop.id}",
            600,
            json.dumps(capacity)
        )
```

---

## 5. 监控告警

### 5.1 系统监控

**监控指标**:
```yaml
# monitoring.yml
metrics:
  system:
    - cpu_usage
    - memory_usage
    - disk_usage
    - network_io
  
  application:
    - api_response_time
    - api_error_rate
    - active_users
    - cache_hit_rate
  
  business:
    - work_orders_count
    - production_efficiency
    - defect_rate
    - on_time_delivery_rate
```

**告警规则**:
```yaml
# alerts.yml
alerts:
  - name: "API响应慢"
    metric: api_response_time
    threshold: 1000ms
    duration: 5m
    severity: warning
    
  - name: "CPU过高"
    metric: cpu_usage
    threshold: 80%
    duration: 10m
    severity: critical
    
  - name: "数据库连接池耗尽"
    metric: db_pool_usage
    threshold: 90%
    duration: 1m
    severity: critical
```

### 5.2 业务监控

**生产监控看板**:
```python
# 关键指标实时监控
def production_health_check():
    return {
        'active_work_orders': count_active_orders(),
        'avg_progress': calculate_avg_progress(),
        'defect_rate_today': calculate_defect_rate(),
        'capacity_utilization': calculate_utilization(),
        'delayed_orders': count_delayed_orders(),
        'quality_alerts': count_quality_alerts()
    }
```

---

## 6. 安全管理

### 6.1 访问控制

**IP白名单**:
```nginx
# nginx.conf
location /api/admin {
    allow 192.168.1.0/24;  # 内网
    allow 10.0.0.100;      # VPN
    deny all;
}
```

**API访问限流**:
```python
# rate_limiting.py
from flask_limiter import Limiter

limiter = Limiter(
    key_func=lambda: request.remote_addr,
    default_limits=["200 per day", "50 per hour"]
)

@app.route('/api/production/work-orders')
@limiter.limit("10 per minute")
def get_work_orders():
    ...
```

### 6.2 数据加密

**敏感数据加密**:
```python
from cryptography.fernet import Fernet

# 成本数据加密
def encrypt_cost(cost_value):
    key = load_encryption_key()
    f = Fernet(key)
    encrypted = f.encrypt(str(cost_value).encode())
    return encrypted.decode()
```

### 6.3 审计日志

```sql
-- 审计日志表
CREATE TABLE audit_log (
    id INT PRIMARY KEY AUTO_INCREMENT,
    user_id INT,
    action VARCHAR(50),
    table_name VARCHAR(50),
    record_id INT,
    old_value JSON,
    new_value JSON,
    ip_address VARCHAR(50),
    created_at DATETIME
);

-- 查询审计日志
SELECT * FROM audit_log
WHERE user_id = 15
AND action = 'UPDATE'
AND created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY);
```

---

## 7. 故障排查

### 7.1 常见问题

**问题1: 系统响应慢**
```bash
# 检查CPU/内存
top
free -m

# 检查数据库慢查询
mysql> SHOW PROCESSLIST;
mysql> SHOW FULL PROCESSLIST;

# 检查Redis
redis-cli INFO stats
redis-cli SLOWLOG GET 10

# 检查应用日志
tail -f /var/log/production/app.log
```

**问题2: 数据不一致**
```python
# 数据一致性检查脚本
def check_data_consistency():
    # 1. 检查工单数量
    work_orders = WorkOrder.query.all()
    for wo in work_orders:
        reports_sum = sum(r.completed_qty for r in wo.work_reports)
        if wo.completed_qty != reports_sum:
            print(f"⚠️ 工单{wo.id}数量不一致")
    
    # 2. 检查质检数据
    inspections = QualityInspection.query.all()
    for qi in inspections:
        if qi.qualified_qty + qi.defect_qty != qi.inspection_qty:
            print(f"⚠️ 质检{qi.id}数量不符")
```

### 7.2 日志分析

```bash
# 查找错误日志
grep "ERROR" /var/log/production/app.log | tail -100

# 统计API调用频率
awk '{print $7}' /var/log/nginx/access.log | sort | uniq -c | sort -rn | head -20

# 分析响应时间
awk '{print $NF}' /var/log/nginx/access.log | \
  awk '{sum+=$1; count+=1} END {print "Avg:", sum/count}'
```

---

## 8. 版本升级

### 8.1 升级准备

**升级前检查**:
- ✅ 完整备份数据库
- ✅ 备份配置文件
- ✅ 测试环境验证
- ✅ 通知用户维护时间
- ✅ 准备回滚方案

### 8.2 升级步骤

```bash
#!/bin/bash
# upgrade.sh

# 1. 备份
./backup.sh

# 2. 停止服务
systemctl stop production-app

# 3. 更新代码
git pull origin main

# 4. 安装依赖
pip install -r requirements.txt

# 5. 数据库迁移
alembic upgrade head

# 6. 重启服务
systemctl start production-app

# 7. 健康检查
curl http://localhost:8000/health
```

---

## 9. 最佳实践

✅ 定期备份（每日）
✅ 监控告警24/7
✅ 定期审查权限
✅ 优化慢查询
✅ 清理历史数据
✅ 更新文档

---

## 10. 技术支持

- 技术支持: support@company.com
- 紧急电话: 400-xxx-xxxx
- 在线文档: https://docs.company.com

---

## 11. 版本历史

| 版本 | 日期 | 说明 |
|------|------|------|
| v1.0 | 2026-02-16 | 初始版本 |

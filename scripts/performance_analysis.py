#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½åˆ†æå’Œä¼˜åŒ–è„šæœ¬
"""

import os
import sys
import time
from pathlib import Path
from typing import Any, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import text
from sqlalchemy.orm import Session

from app.api import deps


def analyze_database_performance() -> Dict[str, Any]:
    """åˆ†ææ•°æ®åº“æ€§èƒ½"""
    db = next(deps.get_db())

    results = {}

    try:
        # 1. åˆ†æå¤§è¡¨
        large_tables = db.execute(
            text("""
            SELECT
                schemaname,
                tablename,
                attname,
                n_distinct,
                correlation
            FROM pg_stats
            WHERE schemaname = 'public'
            ORDER BY tablename, attname;
        """)
        ).fetchall()

        results["large_tables"] = large_tables[:20]  # å–å‰20ä¸ªæœ€å¤§çš„è¡¨

        # 2. åˆ†ææ…¢æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰ï¼‰
        slow_queries = (
            db.execute(
                text("""
            SELECT query, mean_time, calls, total_time
            FROM pg_stat_statements
            ORDER BY mean_time DESC
            LIMIT 10;
        """)
            ).fetchall()
            if has_pg_stat_statements(db)
            else []
        )

        results["slow_queries"] = slow_queries

        # 3. åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
        index_usage = db.execute(
            text("""
            SELECT
                schemaname,
                tablename,
                indexname,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch
            FROM pg_stat_user_indexes
            ORDER BY idx_scan DESC;
        """)
        ).fetchall()

        results["index_usage"] = index_usage

        # 4. æ£€æŸ¥ç¼ºå¤±çš„ç´¢å¼•
        missing_indexes = db.execute(
            text("""
            SELECT
                schemaname,
                tablename,
                attname,
                n_distinct,
                correlation
            FROM pg_stats
            WHERE schemaname = 'public'
              AND tablename IN ('project', 'issue', 'alert_record', 'shortage_report')
              AND attname NOT IN ('id', 'created_at', 'updated_at')
            ORDER BY n_distinct DESC;
        """)
        ).fetchall()

        results["missing_indexes"] = missing_indexes

    except Exception as e:
        results["error"] = str(e)
    finally:
        db.close()

    return results


def has_pg_stat_statements(db: Session) -> bool:
    """æ£€æŸ¥æ˜¯å¦æœ‰pg_stat_statementsæ‰©å±•"""
    try:
        db.execute(
            text("SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements'")
        )
        return True
    except Exception:
        return False


def analyze_code_complexity() -> Dict[str, Any]:
    """åˆ†æä»£ç å¤æ‚åº¦"""
    results = {}

    # åˆ†æå¤§å‹æ–‡ä»¶
    app_dir = project_root / "app"

    large_files = []
    for file_path in app_dir.rglob("*.py"):
        if "migrations" in str(file_path):
            continue

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                lines = len(f.readlines())
                if lines > 300:  # å¤§äº300è¡Œçš„æ–‡ä»¶
                    large_files.append(
                        {
                            "file": str(file_path.relative_to(project_root)),
                            "lines": lines,
                            "size_mb": os.path.getsize(file_path) / 1024 / 1024,
                        }
                    )
        except (OSError, UnicodeDecodeError):
            pass

    results["large_files"] = sorted(
        large_files, key=lambda x: x["lines"], reverse=True
    )[:20]

    # åˆ†æå‡½æ•°å¤æ‚åº¦
    complex_functions = []
    for file_path in app_dir.rglob("*.py"):
        if "migrations" in str(file_path):
            continue

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                lines = content.split("\n")

                for i, line in enumerate(lines):
                    if "def " in line and ":" in line:
                        # ç®€å•è®¡ç®—å‡½æ•°è¡Œæ•°
                        func_lines = 0
                        indent_level = len(line) - len(line.lstrip())

                        for j in range(i + 1, len(lines)):
                            if lines[j].strip() == "":
                                continue
                            current_indent = len(lines[j]) - len(lines[j].lstrip())
                            if current_indent <= indent_level and lines[j].strip():
                                break
                            func_lines += 1

                        if func_lines > 50:  # è¶…è¿‡50è¡Œçš„å‡½æ•°
                            complex_functions.append(
                                {
                                    "file": str(file_path.relative_to(project_root)),
                                    "function": line.strip(),
                                    "line_number": i + 1,
                                    "complexity_lines": func_lines,
                                }
                            )
        except (OSError, UnicodeDecodeError, IndexError):
            pass

    results["complex_functions"] = sorted(
        complex_functions, key=lambda x: x["complexity_lines"], reverse=True
    )[:15]

    return results


def generate_performance_report() -> str:
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    report = []
    report.append("# ğŸ“Š ä»£ç è´¨é‡å’Œæ€§èƒ½åˆ†ææŠ¥å‘Š")
    report.append(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    report.append("")

    # æ•°æ®åº“æ€§èƒ½åˆ†æ
    db_results = analyze_database_performance()
    report.append("## ğŸ—„ï¸ æ•°æ®åº“æ€§èƒ½åˆ†æ")

    if "large_tables" in db_results:
        report.append("### ğŸ“‹ å¤§å‹è¡¨ç»Ÿè®¡")
        report.append("| è¡¨å | å­—æ®µ | å”¯ä¸€å€¼æ•° | ç›¸å…³æ€§ |")
        report.append("|------|------|----------|--------|")
        for table in db_results["large_tables"][:10]:
            report.append(
                f"| {table.tablename} | {table.attname} | {table.n_distinct} | {table.correlation:.2f} |"
            )
        report.append("")

    if "slow_queries" in db_results:
        report.append("### ğŸŒ æ…¢æŸ¥è¯¢åˆ†æ")
        if db_results["slow_queries"]:
            report.append("| æŸ¥è¯¢ | å¹³å‡æ—¶é—´(ms) | è°ƒç”¨æ¬¡æ•° | æ€»æ—¶é—´(ms) |")
            report.append("|------|---------------|----------|------------|")
            for query in db_results["slow_queries"]:
                report.append(
                    f"| `{query.query[:50]}...` | {query.mean_time:.2f} | {query.calls} | {query.total_time:.2f} |"
                )
        else:
            report.append("âœ… æœªå‘ç°æ˜æ˜¾çš„æ…¢æŸ¥è¯¢")
        report.append("")

    # ä»£ç å¤æ‚åº¦åˆ†æ
    code_results = analyze_code_complexity()
    report.append("## ğŸ’» ä»£ç å¤æ‚åº¦åˆ†æ")

    report.append("### ğŸ“ å¤§å‹æ–‡ä»¶ (>300è¡Œ)")
    report.append("| æ–‡ä»¶ | è¡Œæ•° | å¤§å°(MB) |")
    report.append("|------|------|----------|")
    for file_info in code_results["large_files"][:10]:
        report.append(
            f"| {file_info['file']} | {file_info['lines']} | {file_info['size_mb']:.2f} |"
        )
    report.append("")

    report.append("### ğŸ”§ å¤æ‚å‡½æ•° (>50è¡Œ)")
    report.append("| æ–‡ä»¶ | å‡½æ•° | è¡Œå· | å¤æ‚åº¦ |")
    report.append("|------|------|------|--------|")
    for func_info in code_results["complex_functions"][:10]:
        report.append(
            f"| {func_info['file']} | {func_info['function']} | {func_info['line_number']} | {func_info['complexity_lines']} |"
        )
    report.append("")

    # ä¼˜åŒ–å»ºè®®
    report.append("## ğŸš€ ä¼˜åŒ–å»ºè®®")
    report.append("")
    report.append("### ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–")
    report.append("1. **æ•°æ®åº“ä¼˜åŒ–**:")
    report.append("   - ä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µæ·»åŠ ç´¢å¼•")
    report.append("   - ä¼˜åŒ–å¤æ‚æŸ¥è¯¢ï¼Œé¿å…N+1é—®é¢˜")
    report.append("   - è€ƒè™‘è¯»å†™åˆ†ç¦»å’Œç¼“å­˜")
    report.append("")
    report.append("2. **ä»£ç ä¼˜åŒ–**:")
    report.append("   - æ‹†åˆ†å¤§å‹å‡½æ•°ï¼Œæé«˜å¯è¯»æ€§")
    report.append("   - æå–é‡å¤ä»£ç åˆ°å…¬å…±æ¨¡å—")
    report.append("   - ä½¿ç”¨å¼‚æ­¥å¤„ç†æå‡å“åº”é€Ÿåº¦")
    report.append("")

    report.append("### ğŸ›¡ï¸ å®‰å…¨ä¼˜åŒ–")
    report.append("1. **è¾“å…¥éªŒè¯**:")
    report.append("   - åŠ å¼ºAPIå‚æ•°éªŒè¯")
    report.append("   - é˜²æ­¢SQLæ³¨å…¥æ”»å‡»")
    report.append("   - æ·»åŠ è¯·æ±‚é¢‘ç‡é™åˆ¶")
    report.append("")

    report.append("2. **æ•°æ®ä¿æŠ¤**:")
    report.append("   - æ•æ„Ÿæ•°æ®åŠ å¯†å­˜å‚¨")
    report.append("   - å®æ–½è®¿é—®æ§åˆ¶")
    report.append("   - å®šæœŸå®‰å…¨å®¡è®¡")
    report.append("")

    return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹æ€§èƒ½åˆ†æ...")

    try:
        # ç”ŸæˆæŠ¥å‘Š
        report = generate_performance_report()

        # ä¿å­˜æŠ¥å‘Š
        report_path = project_root / "performance_analysis_report.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)

        print(f"âœ… æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

        # è¾“å‡ºå…³é”®æŒ‡æ ‡
        print("\nğŸ“Š å…³é”®æŒ‡æ ‡:")
        code_results = analyze_code_complexity()
        print(f"- å¤§å‹æ–‡ä»¶æ•°é‡: {len(code_results['large_files'])}")
        print(f"- å¤æ‚å‡½æ•°æ•°é‡: {len(code_results['complex_functions'])}")

        if code_results["large_files"]:
            max_file = max(code_results["large_files"], key=lambda x: x["lines"])
            print(f"- æœ€å¤§æ–‡ä»¶: {max_file['file']} ({max_file['lines']}è¡Œ)")

        if code_results["complex_functions"]:
            max_func = max(
                code_results["complex_functions"], key=lambda x: x["complexity_lines"]
            )
            print(
                f"- æœ€å¤æ‚å‡½æ•°: {max_func['function']} ({max_func['complexity_lines']}è¡Œ)"
            )

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•

å¯¹æ¯”ä¼˜åŒ–å‰åŽçš„æ€§èƒ½å·®å¼‚
"""

import json
import os
import statistics

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List

from sqlalchemy.orm import Session

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.models.alert import AlertRecord
from app.models.base import get_db_session
from app.models.project import Project
from app.services.cache.business_cache import get_business_cache
from app.services.database.query_optimizer import QueryOptimizer


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""

    def __init__(self):
        self.results = {}
        self.db = None

    def setup(self):
        """è®¾ç½®æµ‹è¯•çŽ¯å¢ƒ"""
        self.db = get_db_session()
        print("ðŸ”§ æ€§èƒ½æµ‹è¯•çŽ¯å¢ƒåˆå§‹åŒ–å®Œæˆ")

    def teardown(self):
        """æ¸…ç†æµ‹è¯•çŽ¯å¢ƒ"""
        if self.db:
            self.db.close()
            print("ðŸ”§ æ€§èƒ½æµ‹è¯•çŽ¯å¢ƒæ¸…ç†å®Œæˆ")

    def measure_time(self, func, *args, **kwargs):
        """
        æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´

        Returns:
            tuple: (æ‰§è¡Œæ—¶é—´, æ‰§è¡Œç»“æžœ)
        """
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        execution_time = end_time - start_time
        return execution_time, result

    def benchmark_project_list(self, iterations: int = 10):
        """åŸºå‡†æµ‹è¯•ï¼šé¡¹ç›®åˆ—è¡¨æŸ¥è¯¢"""
        print("\nðŸ“Š åŸºå‡†æµ‹è¯•ï¼šé¡¹ç›®åˆ—è¡¨æŸ¥è¯¢")

        # æµ‹è¯•åŽŸå§‹æŸ¥è¯¢
        original_times = []
        for i in range(iterations):
            def original_query():
                return self.db.query(Project).options(
                    # æ²¡æœ‰ä¼˜åŒ–çš„åŽŸå§‹æŸ¥è¯¢
                ).limit(100).all()

            exec_time, _ = self.measure_time(original_query)
            original_times.append(exec_time)
            print(f"  åŽŸå§‹æŸ¥è¯¢ {i+1}: {exec_time:.3f}s")

        # æµ‹è¯•ä¼˜åŒ–æŸ¥è¯¢
        optimized_times = []
        optimizer = QueryOptimizer(self.db)
        for i in range(iterations):
            def optimized_query():
                return optimizer.get_project_list_optimized(skip=0, limit=100)

            exec_time, _ = self.measure_time(optimized_query)
            optimized_times.append(exec_time)
            print(f"  ä¼˜åŒ–æŸ¥è¯¢ {i+1}: {exec_time:.3f}s")

        # æµ‹è¯•ç¼“å­˜æŸ¥è¯¢
        cached_times = []
        business_cache = get_business_cache()
        for i in range(iterations):
            def cached_query():
                # å…ˆæ¸…é™¤ç¼“å­˜ç¡®ä¿ç¬¬ä¸€æ¬¡æŸ¥è¯¢ä»Žæ•°æ®åº“èŽ·å–
                if i == 0:
                    from app.services.cache.redis_cache import CacheManager
                    CacheManager.clear_project_cache()

                cached_projects = business_cache.get_project_list(0, 100)
                if cached_projects is None:
                    # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡ŒæŸ¥è¯¢å¹¶ç¼“å­˜
                    projects = optimizer.get_project_list_optimized(0, 100)
                    business_cache.set_project_list(projects, 0, 100)
                    return projects
                return cached_projects

            exec_time, _ = self.measure_time(cached_query)
            cached_times.append(exec_time)
            print(f"  ç¼“å­˜æŸ¥è¯¢ {i+1}: {exec_time:.3f}s")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.results['project_list'] = {
            'original': {
                'avg': statistics.mean(original_times),
                'min': min(original_times),
                'max': max(original_times),
                'std': statistics.stdev(original_times) if len(original_times) > 1 else 0
            },
            'optimized': {
                'avg': statistics.mean(optimized_times),
                'min': min(optimized_times),
                'max': max(optimized_times),
                'std': statistics.stdev(optimized_times) if len(optimized_times) > 1 else 0
            },
            'cached': {
                'avg': statistics.mean(cached_times),
                'min': min(cached_times),
                'max': max(cached_times),
                'std': statistics.stdev(cached_times) if len(cached_times) > 1 else 0
            }
        }

        # è®¡ç®—æ€§èƒ½æå‡
        original_avg = self.results['project_list']['original']['avg']
        optimized_avg = self.results['project_list']['optimized']['avg']
        cached_avg = self.results['project_list']['cached']['avg']

        improvement_optimized = ((original_avg - optimized_avg) / original_avg) * 100
        improvement_cached = ((original_avg - cached_avg) / original_avg) * 100

        print(f"\nðŸ“ˆ æ€§èƒ½æå‡ç»Ÿè®¡:")
        print(f"  åŽŸå§‹æŸ¥è¯¢å¹³å‡æ—¶é—´: {original_avg:.3f}s")
        print(f"  ä¼˜åŒ–æŸ¥è¯¢å¹³å‡æ—¶é—´: {optimized_avg:.3f}s (æå‡ {improvement_optimized:.1f}%)")
        print(f"  ç¼“å­˜æŸ¥è¯¢å¹³å‡æ—¶é—´: {cached_avg:.3f}s (æå‡ {improvement_cached:.1f}%)")

    def benchmark_alert_statistics(self, iterations: int = 5):
        """åŸºå‡†æµ‹è¯•ï¼šå‘Šè­¦ç»Ÿè®¡æŸ¥è¯¢"""
        print("\nðŸ“Š åŸºå‡†æµ‹è¯•ï¼šå‘Šè­¦ç»Ÿè®¡æŸ¥è¯¢")

        # æµ‹è¯•åŽŸå§‹ç»Ÿè®¡æŸ¥è¯¢
        original_times = []
        for i in range(iterations):
            def original_stats():
                # æ¨¡æ‹ŸåŽŸå§‹çš„ç»Ÿè®¡æŸ¥è¯¢ï¼ˆå¯èƒ½å­˜åœ¨æ€§èƒ½é—®é¢˜ï¼‰
                return self.db.query(AlertRecord).filter(
                    AlertRecord.created_at >= datetime.now() - timedelta(days=30)
                ).all()

            exec_time, _ = self.measure_time(original_stats)
            original_times.append(exec_time)
            print(f"  åŽŸå§‹ç»Ÿè®¡ {i+1}: {exec_time:.3f}s")

        # æµ‹è¯•ä¼˜åŒ–ç»Ÿè®¡æŸ¥è¯¢
        optimized_times = []
        optimizer = QueryOptimizer(self.db)
        for i in range(iterations):
            def optimized_stats():
                return optimizer.get_alert_statistics_optimized(days=30)

            exec_time, _ = self.measure_time(optimized_stats)
            optimized_times.append(exec_time)
            print(f"  ä¼˜åŒ–ç»Ÿè®¡ {i+1}: {exec_time:.3f}s")

        # æµ‹è¯•ç¼“å­˜ç»Ÿè®¡æŸ¥è¯¢
        cached_times = []
        business_cache = get_business_cache()
        for i in range(iterations):
            def cached_stats():
                stats = business_cache.get_alert_statistics(30)
                if stats is None:
                    stats = optimizer.get_alert_statistics_optimized(30)
                    business_cache.set_alert_statistics(stats, 30)
                return stats

            exec_time, _ = self.measure_time(cached_stats)
            cached_times.append(exec_time)
            print(f"  ç¼“å­˜ç»Ÿè®¡ {i+1}: {exec_time:.3f}s")

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.results['alert_statistics'] = {
            'original': {
                'avg': statistics.mean(original_times),
                'min': min(original_times),
                'max': max(original_times),
                'std': statistics.stdev(original_times) if len(original_times) > 1 else 0
            },
            'optimized': {
                'avg': statistics.mean(optimized_times),
                'min': min(optimized_times),
                'max': max(optimized_times),
                'std': statistics.stdev(optimized_times) if len(optimized_times) > 1 else 0
            },
            'cached': {
                'avg': statistics.mean(cached_times),
                'min': min(cached_times),
                'max': max(cached_times),
                'std': statistics.stdev(cached_times) if len(cached_times) > 1 else 0
            }
        }

        # è®¡ç®—æ€§èƒ½æå‡
        original_avg = self.results['alert_statistics']['original']['avg']
        optimized_avg = self.results['alert_statistics']['optimized']['avg']
        cached_avg = self.results['alert_statistics']['cached']['avg']

        improvement_optimized = ((original_avg - optimized_avg) / original_avg) * 100
        improvement_cached = ((original_avg - cached_avg) / original_avg) * 100

        print(f"\nðŸ“ˆ å‘Šè­¦ç»Ÿè®¡æ€§èƒ½æå‡:")
        print(f"  åŽŸå§‹ç»Ÿè®¡å¹³å‡æ—¶é—´: {original_avg:.3f}s")
        print(f"  ä¼˜åŒ–ç»Ÿè®¡å¹³å‡æ—¶é—´: {optimized_avg:.3f}s (æå‡ {improvement_optimized:.1f}%)")
        print(f"  ç¼“å­˜ç»Ÿè®¡å¹³å‡æ—¶é—´: {cached_avg:.3f}s (æå‡ {improvement_cached:.1f}%)")

    def benchmark_database_connections(self):
        """åŸºå‡†æµ‹è¯•ï¼šæ•°æ®åº“è¿žæŽ¥æ± æ€§èƒ½"""
        print("\nðŸ“Š åŸºå‡†æµ‹è¯•ï¼šæ•°æ®åº“è¿žæŽ¥æ€§èƒ½")

        connection_times = []
        for i in range(20):
            def test_connection():
                try:
                    with self.db.begin():
                        self.db.execute("SELECT 1")
                    return True
                except Exception:
                    return False

            exec_time, success = self.measure_time(test_connection)
            connection_times.append(exec_time)
            print(f"  è¿žæŽ¥æµ‹è¯• {i+1}: {exec_time:.3f}s - {'æˆåŠŸ' if success else 'å¤±è´¥'}")

        avg_time = statistics.mean(connection_times)
        print(f"\nðŸ“ˆ è¿žæŽ¥æ€§èƒ½ç»Ÿè®¡:")
        print(f"  å¹³å‡è¿žæŽ¥æ—¶é—´: {avg_time:.3f}s")
        print(f"  æœ€å¿«è¿žæŽ¥æ—¶é—´: {min(connection_times):.3f}s")
        print(f"  æœ€æ…¢è¿žæŽ¥æ—¶é—´: {max(connection_times):.3f}s")

    def benchmark_complexity_reduction(self):
        """åŸºå‡†æµ‹è¯•ï¼šå¤æ‚å‡½æ•°æ‹†åˆ†åŽçš„æ€§èƒ½"""
        print("\nðŸ“Š åŸºå‡†æµ‹è¯•ï¼šå‡½æ•°å¤æ‚åº¦ä¼˜åŒ–")

        # æµ‹è¯•é‡æž„å‰çš„æ”¶æ¬¾è®¡åˆ’ç”Ÿæˆ
        print("  æµ‹è¯•æ”¶æ¬¾è®¡åˆ’ç”Ÿæˆ...")

        try:
            from app.api.v1.endpoints.sales.contracts import (
                _generate_payment_plans_from_contract,
            )
            from app.models.sales import Contract

            # èŽ·å–æµ‹è¯•åˆåŒ
            test_contract = self.db.query(Contract).first()
            if test_contract:
                # æµ‹è¯•åŽŸå§‹å‡½æ•°ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
                original_times = []
                for i in range(3):
                    def original_payment():
                        return _generate_payment_plans_from_contract(self.db, test_contract)

                    exec_time, _ = self.measure_time(original_payment)
                    original_times.append(exec_time)
                    print(f"    é‡æž„åŽå‡½æ•° {i+1}: {exec_time:.3f}s")

                avg_time = statistics.mean(original_times)
                print(f"    å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.3f}s")

                self.results['payment_plans'] = {
                    'refactored': {
                        'avg': avg_time,
                        'min': min(original_times),
                        'max': max(original_times)
                    }
                }
        except Exception as e:
            print(f"    æ”¶æ¬¾è®¡åˆ’æµ‹è¯•è·³è¿‡: {str(e)}")

    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("ðŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 50)

        try:
            self.setup()

            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.benchmark_project_list()
            self.benchmark_alert_statistics()
            self.benchmark_database_connections()
            self.benchmark_complexity_reduction()

            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            self.generate_report()

        except Exception as e:
            print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            self.teardown()

    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 50)
        print("ðŸ“Š æ€§èƒ½ä¼˜åŒ–ç»¼åˆæŠ¥å‘Š")
        print("=" * 50)

        if not self.results:
            print("âŒ æ²¡æœ‰æµ‹è¯•ç»“æžœæ•°æ®")
            return

        # ç”ŸæˆæŠ¥å‘Šæ•°æ®
        report_data = {
            'timestamp': datetime.now().isoformat(),
            'results': self.results,
            'summary': self.generate_summary()
        }

        # ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
        report_file = f"performance_benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        print(f"\nðŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        # æ‰“å°æ€»ç»“
        print("\nðŸ“ˆ ä¼˜åŒ–æ•ˆæžœæ€»ç»“:")
        for test_name, data in self.results.items():
            if 'original' in data and 'optimized' in data:
                original_time = data['original']['avg']
                optimized_time = data['optimized']['avg']
                improvement = ((original_time - optimized_time) / original_time) * 100
                print(f"  {test_name}: æå‡ {improvement:.1f}% ({original_time:.3f}s â†’ {optimized_time:.3f}s)")

    def generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–æ€»ç»“"""
        summary = {
            'total_tests': len(self.results),
            'significant_improvements': 0,
            'overall_improvement': 0
        }

        total_improvement = 0
        test_count = 0

        for test_name, data in self.results.items():
            if 'original' in data and 'optimized' in data:
                original_time = data['original']['avg']
                optimized_time = data['optimized']['avg']
                improvement = ((original_time - optimized_time) / original_time) * 100

                total_improvement += improvement
                test_count += 1

                if improvement > 10:  # è¶…è¿‡10%è®¤ä¸ºæ˜¾è‘—æ”¹è¿›
                    summary['significant_improvements'] += 1

        if test_count > 0:
            summary['overall_improvement'] = total_improvement / test_count

        return summary


def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    benchmark.run_all_benchmarks()


if __name__ == "__main__":
    main()

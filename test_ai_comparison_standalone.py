#!/usr/bin/env python3
"""
AI æ¨¡å‹å¯¹æ¯”æµ‹è¯•ï¼ˆç‹¬ç«‹ç‰ˆæœ¬ï¼‰
ç›´æ¥æµ‹è¯• GLM-5, Kimi, GPT-4ï¼ˆMockæ¨¡å¼ï¼‰
"""

import os
import time
import json
from datetime import datetime
from typing import Dict, List

# Mock AI å®¢æˆ·ç«¯æœåŠ¡
class MockAIClient:
    """Mock AI å®¢æˆ·ç«¯ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
    
    def __init__(self):
        self.models_config = {
            'glm-5': {
                'avg_time': 3.5,
                'tokens_multiplier': 1.2,
                'quality_base': 4.0,
                'cost_per_1k': 0.10
            },
            'kimi': {
                'avg_time': 2.8,
                'tokens_multiplier': 0.8,
                'quality_base': 3.7,
                'cost_per_1k': 0.08
            },
            'gpt-4': {
                'avg_time': 4.2,
                'tokens_multiplier': 1.5,
                'quality_base': 4.3,
                'cost_per_1k': 0.30
            }
        }
    
    def generate_solution(self, prompt: str, model: str, **kwargs) -> Dict:
        """æ¨¡æ‹Ÿç”Ÿæˆæ–¹æ¡ˆ"""
        import random
        
        config = self.models_config.get(model, self.models_config['glm-5'])
        
        # æ¨¡æ‹Ÿå“åº”æ—¶é—´
        time.sleep(config['avg_time'] * random.uniform(0.8, 1.2))
        
        # æ¨¡æ‹ŸTokenæ¶ˆè€—
        prompt_tokens = len(prompt) // 4
        completion_tokens = int(prompt_tokens * config['tokens_multiplier'])
        total_tokens = prompt_tokens + completion_tokens
        
        # ç”ŸæˆMockå†…å®¹
        content = self._generate_mock_content(prompt, model)
        
        return {
            'content': content,
            'model': f"{model}-mock",
            'usage': {
                'prompt_tokens': prompt_tokens,
                'completion_tokens': completion_tokens,
                'total_tokens': total_tokens
            }
        }
    
    def _generate_mock_content(self, prompt: str, model: str) -> str:
        """ç”ŸæˆMockå†…å®¹"""
        if 'éœ€æ±‚' in prompt or 'åˆ†æ' in prompt:
            return f"""
# éœ€æ±‚åˆ†ææŠ¥å‘Š (ç”± {model.upper()} ç”Ÿæˆ)

## æ ¸å¿ƒéœ€æ±‚
- **è¡Œä¸š**: æ±½è½¦åˆ¶é€ 
- **äº§èƒ½**: 100ä»¶/å°æ—¶
- **è‡ªåŠ¨åŒ–ç¨‹åº¦**: 95%
- **å…³é”®æŠ€æœ¯**: è§†è§‰æ£€æµ‹ã€æœºå™¨äººè£…é…

## æŠ€æœ¯è¦æ±‚
1. é«˜ç²¾åº¦è§†è§‰æ£€æµ‹ç³»ç»Ÿ
2. å…­è½´æœºå™¨äººè£…é…å•å…ƒ
3. PLCæ§åˆ¶ç³»ç»Ÿ
4. æ•°æ®é‡‡é›†ä¸ç›‘æ§

## é¢„ä¼°è§„æ¨¡
- è®¾å¤‡æŠ•èµ„: Â¥300-400ä¸‡
- å·¥æœŸ: 4-6ä¸ªæœˆ
- äººå‘˜éœ€æ±‚: 5-8äºº

## é£é™©è¯„ä¼°
- æŠ€æœ¯éš¾åº¦: ä¸­ç­‰
- æ—¶é—´é£é™©: ä½
- æˆæœ¬é£é™©: ä¸­
"""
        
        elif 'æ–¹æ¡ˆ' in prompt or 'æ¶æ„' in prompt:
            return f"""
# æŠ€æœ¯æ–¹æ¡ˆ (ç”± {model.upper()} ç”Ÿæˆ)

## ç³»ç»Ÿæ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MESç³»ç»Ÿ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PLCæ§åˆ¶ç³»ç»Ÿ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚ è§†è§‰  â”‚ â”‚ æœºå™¨äººâ”‚
â”‚ æ£€æµ‹  â”‚ â”‚ è£…é…  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

## è®¾å¤‡æ¸…å•
1. **è§†è§‰æ£€æµ‹ç³»ç»Ÿ** (2å¥—) - Â¥80ä¸‡
2. **å…­è½´æœºå™¨äºº** (3å°) - Â¥150ä¸‡
3. **PLCæ§åˆ¶æŸœ** (1å¥—) - Â¥30ä¸‡
4. **è¾“é€çº¿ç³»ç»Ÿ** (1å¥—) - Â¥40ä¸‡

## å·¥è‰ºæµç¨‹
1. ä¸Šæ–™ â†’ 2. è§†è§‰å®šä½ â†’ 3. æœºå™¨äººæŠ“å– â†’ 4. è£…é… â†’ 5. æ£€æµ‹ â†’ 6. ä¸‹æ–™

## æŠ€æœ¯å‚æ•°
- èŠ‚æ‹: 36ç§’/ä»¶
- ç²¾åº¦: Â±0.05mm
- è‡ªåŠ¨åŒ–ç‡: 95%
"""
        
        elif 'æˆæœ¬' in prompt or 'é¢„ç®—' in prompt:
            return f"""
# æˆæœ¬ä¼°ç®—æŠ¥å‘Š (ç”± {model.upper()} ç”Ÿæˆ)

## è®¾å¤‡æˆæœ¬
- è§†è§‰ç³»ç»Ÿ: Â¥80ä¸‡
- æœºå™¨äºº: Â¥150ä¸‡
- æ§åˆ¶ç³»ç»Ÿ: Â¥30ä¸‡
- è¾“é€ç³»ç»Ÿ: Â¥40ä¸‡
- **å°è®¡**: Â¥300ä¸‡

## å®æ–½æˆæœ¬
- ç³»ç»Ÿé›†æˆ: Â¥50ä¸‡
- ç°åœºå®‰è£…: Â¥20ä¸‡
- è°ƒè¯•åŸ¹è®­: Â¥15ä¸‡
- **å°è®¡**: Â¥85ä¸‡

## å…¶ä»–è´¹ç”¨
- è®¾è®¡è´¹: Â¥10ä¸‡
- å·®æ—…è´¹: Â¥5ä¸‡
- **å°è®¡**: Â¥15ä¸‡

## æ€»è®¡
**Â¥400ä¸‡** (å«ç¨)
"""
        
        elif 'æŠ¥ä»·' in prompt:
            return f"""
# å•†åŠ¡æŠ¥ä»·å• (ç”± {model.upper()} ç”Ÿæˆ)

é¡¹ç›®åç§°: æ±½è½¦é›¶éƒ¨ä»¶è£…é…çº¿
æŠ¥ä»·æ—¥æœŸ: 2026-02-15

## è®¾å¤‡æ˜ç»†
| åºå· | åç§° | è§„æ ¼å‹å· | æ•°é‡ | å•ä»·(ä¸‡) | é‡‘é¢(ä¸‡) |
|------|------|---------|------|---------|---------|
| 1 | è§†è§‰æ£€æµ‹ç³»ç»Ÿ | VIS-2000 | 2 | 40 | 80 |
| 2 | å…­è½´æœºå™¨äºº | ROBOT-600 | 3 | 50 | 150 |
| 3 | PLCæ§åˆ¶ç³»ç»Ÿ | S7-1500 | 1 | 30 | 30 |
| 4 | è¾“é€çº¿ç³»ç»Ÿ | CONV-100 | 1 | 40 | 40 |

è®¾å¤‡å°è®¡: Â¥300ä¸‡

## å·¥ç¨‹æœåŠ¡
- ç³»ç»Ÿé›†æˆ: Â¥50ä¸‡
- å®‰è£…è°ƒè¯•: Â¥35ä¸‡
- åŸ¹è®­æœåŠ¡: Â¥15ä¸‡

æœåŠ¡å°è®¡: Â¥100ä¸‡

## å•†åŠ¡æ¡æ¬¾
- åˆåŒæ€»ä»·: Â¥400ä¸‡ (å«ç¨)
- ä»˜æ¬¾æ–¹å¼: 3-3-3-1
- äº¤è´§æœŸ: 120å¤©
- è´¨ä¿: 12ä¸ªæœˆ

æœ‰æ•ˆæœŸ: 30å¤©
"""
        else:
            return f"Mock response from {model.upper()}: " + prompt[:200]


class ModelComparison:
    """æ¨¡å‹å¯¹æ¯”æµ‹è¯•"""
    
    def __init__(self):
        self.client = MockAIClient()
        self.models = ['glm-5', 'kimi', 'gpt-4']
        self.results = []
    
    def run_comparison(self):
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        print("=" * 70)
        print("ğŸ”¬ AI æ¨¡å‹å¯¹æ¯”æµ‹è¯• (Mock æ¼”ç¤ºç‰ˆ)")
        print("=" * 70)
        print(f"\næµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        print("ğŸ“Œ è¯´æ˜: è¿™æ˜¯æ¼”ç¤ºç‰ˆæœ¬ï¼Œä½¿ç”¨Mockæ•°æ®æ¨¡æ‹ŸçœŸå®APIå“åº”\n")
        
        test_cases = self.load_test_cases()
        
        for idx, test_case in enumerate(test_cases, 1):
            print(f"\n{'='*70}")
            print(f"ğŸ“ æµ‹è¯•ç”¨ä¾‹ {idx}: {test_case['name']}")
            print("=" * 70)
            
            case_result = {
                'case_id': idx,
                'case_name': test_case['name'],
                'models': {}
            }
            
            for model in self.models:
                print(f"\nğŸ¤– æµ‹è¯•æ¨¡å‹: {model.upper()}")
                print("-" * 70)
                
                result = self.test_model(model, test_case)
                case_result['models'][model] = result
                self.print_result(result)
            
            self.results.append(case_result)
            self.compare_case_results(case_result)
        
        self.generate_final_report()
    
    def test_model(self, model: str, test_case: Dict) -> Dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        start_time = time.time()
        
        try:
            response = self.client.generate_solution(
                prompt=test_case['prompt'],
                model=model
            )
            
            elapsed = time.time() - start_time
            tokens = response['usage']['total_tokens']
            
            # è®¡ç®—æˆæœ¬
            cost_per_1k = self.client.models_config[model]['cost_per_1k']
            cost = tokens / 1000 * cost_per_1k
            
            # è´¨é‡è¯„åˆ†
            quality = self.evaluate_quality(
                response['content'],
                test_case
            )
            
            return {
                'success': True,
                'response_time': round(elapsed, 2),
                'tokens': tokens,
                'cost': round(cost, 4),
                'quality': quality,
                'content': response['content']
            }
        
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def evaluate_quality(self, content: str, test_case: Dict) -> float:
        """è¯„ä¼°è´¨é‡"""
        score = 3.0
        
        keywords = test_case.get('keywords', [])
        if keywords:
            matched = sum(1 for kw in keywords if kw in content)
            score += (matched / len(keywords)) * 1.5
        
        if len(content) >= test_case.get('min_length', 500):
            score += 0.5
        
        return min(5.0, round(score, 1))
    
    def print_result(self, result: Dict):
        """æ‰“å°ç»“æœ"""
        if result['success']:
            print(f"  âœ… æˆåŠŸ")
            print(f"  â±ï¸  å“åº”æ—¶é—´: {result['response_time']}ç§’")
            print(f"  ğŸ“Š Tokenæ¶ˆè€—: {result['tokens']}")
            print(f"  ğŸ’° é¢„ä¼°æˆæœ¬: Â¥{result['cost']}")
            print(f"  â­ è´¨é‡è¯„åˆ†: {result['quality']}/5.0")
            print(f"  ğŸ“ å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
        else:
            print(f"  âŒ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    def compare_case_results(self, case_result: Dict):
        """å¯¹æ¯”å•ä¸ªç”¨ä¾‹ç»“æœ"""
        print(f"\n{'='*70}")
        print(f"ğŸ“Š å¯¹æ¯”åˆ†æ: {case_result['case_name']}")
        print("=" * 70)
        
        models_data = []
        for model, result in case_result['models'].items():
            if result['success']:
                models_data.append({
                    'model': model,
                    'time': result['response_time'],
                    'tokens': result['tokens'],
                    'cost': result['cost'],
                    'quality': result['quality']
                })
        
        if not models_data:
            print("âš ï¸  æ‰€æœ‰æ¨¡å‹æµ‹è¯•å¤±è´¥")
            return
        
        # æ‰¾å‡ºæœ€ä¼˜
        fastest = min(models_data, key=lambda x: x['time'])
        cheapest = min(models_data, key=lambda x: x['cost'])
        best_quality = max(models_data, key=lambda x: x['quality'])
        
        # è®¡ç®—æ€§ä»·æ¯”
        for data in models_data:
            data['value'] = data['quality'] / data['cost'] if data['cost'] > 0 else 0
        best_value = max(models_data, key=lambda x: x['value'])
        
        print(f"\nğŸ† æœ€å¿«: {fastest['model'].upper()} ({fastest['time']}ç§’)")
        print(f"ğŸ’µ æœ€ä¾¿å®œ: {cheapest['model'].upper()} (Â¥{cheapest['cost']})")
        print(f"â­ æœ€é«˜è´¨é‡: {best_quality['model'].upper()} ({best_quality['quality']}/5)")
        print(f"ğŸ¯ æœ€ä½³æ€§ä»·æ¯”: {best_value['model'].upper()} ({best_value['value']:.1f})")
    
    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print(f"\n{'='*70}")
        print("ğŸ“Š æœ€ç»ˆå¯¹æ¯”æŠ¥å‘Š")
        print("=" * 70)
        
        # ç»Ÿè®¡
        stats = {model: {
            'total_time': 0,
            'total_tokens': 0,
            'total_cost': 0,
            'total_quality': 0,
            'count': 0
        } for model in self.models}
        
        for case_result in self.results:
            for model, result in case_result['models'].items():
                if result['success']:
                    stats[model]['total_time'] += result['response_time']
                    stats[model]['total_tokens'] += result['tokens']
                    stats[model]['total_cost'] += result['cost']
                    stats[model]['total_quality'] += result['quality']
                    stats[model]['count'] += 1
        
        # æ‰“å°å¯¹æ¯”è¡¨
        print(f"\n{'æ¨¡å‹':<10} {'å¹³å‡æ—¶é—´':<12} {'å¹³å‡Token':<12} {'å¹³å‡æˆæœ¬':<12} {'å¹³å‡è´¨é‡':<12} {'ç»¼åˆå¾—åˆ†':<12}")
        print("-" * 70)
        
        scores = []
        for model, stat in stats.items():
            if stat['count'] > 0:
                avg_time = stat['total_time'] / stat['count']
                avg_tokens = stat['total_tokens'] / stat['count']
                avg_cost = stat['total_cost'] / stat['count']
                avg_quality = stat['total_quality'] / stat['count']
                
                # ç»¼åˆå¾—åˆ†
                quality_score = (avg_quality / 5) * 100
                speed_score = min((2.0 / avg_time * 100), 100) if avg_time > 0 else 0
                cost_score = min((0.10 / avg_cost * 100), 100) if avg_cost > 0 else 0
                
                comprehensive = (
                    quality_score * 0.4 +
                    speed_score * 0.3 +
                    cost_score * 0.3
                )
                
                scores.append({
                    'model': model,
                    'score': comprehensive
                })
                
                print(f"{model.upper():<10} {avg_time:.2f}ç§’{'':<6} {avg_tokens:.0f}{'':<7} Â¥{avg_cost:.4f}{'':<6} {avg_quality:.1f}/5{'':<7} {comprehensive:.1f}/100")
        
        # æ¨è
        if scores:
            best = max(scores, key=lambda x: x['score'])
            print(f"\n{'='*70}")
            print(f"ğŸ† æ¨èæ¨¡å‹: {best['model'].upper()}")
            print(f"ğŸ“Š ç»¼åˆå¾—åˆ†: {best['score']:.1f}/100")
            print("=" * 70)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"ai_comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'test_time': datetime.now().isoformat(),
                'results': self.results,
                'summary': stats,
                'recommendation': best if scores else None
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}\n")
    
    def load_test_cases(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        return [
            {
                'name': 'éœ€æ±‚ç†è§£å¼•æ“',
                'prompt': 'å®¢æˆ·éœ€æ±‚ï¼šæ±½è½¦åˆ¶é€ ä¼ä¸šï¼Œéœ€è¦è‡ªåŠ¨åŒ–è£…é…çº¿ï¼Œäº§èƒ½100ä»¶/å°æ—¶ï¼Œè‡ªåŠ¨åŒ–ç¨‹åº¦95%ä»¥ä¸Šï¼Œé›†æˆè§†è§‰æ£€æµ‹å’Œæœºå™¨äººè£…é…ã€‚è¯·åˆ†æå¹¶æå–å…³é”®éœ€æ±‚ã€‚',
                'keywords': ['æ±½è½¦', 'è£…é…çº¿', '100', '95', 'è§†è§‰', 'æœºå™¨äºº'],
                'min_length': 300
            },
            {
                'name': 'æ–¹æ¡ˆç”Ÿæˆå¼•æ“',
                'prompt': 'åŸºäºæ±½è½¦åˆ¶é€ è¡Œä¸šï¼Œäº§èƒ½100ä»¶/å°æ—¶ï¼Œè‡ªåŠ¨åŒ–ç¨‹åº¦95%ï¼ŒåŒ…å«è§†è§‰æ£€æµ‹å’Œæœºå™¨äººè£…é…çš„éœ€æ±‚ï¼Œç”Ÿæˆå®Œæ•´æŠ€æœ¯æ–¹æ¡ˆï¼ˆç³»ç»Ÿæ¶æ„ã€è®¾å¤‡æ¸…å•ã€å·¥è‰ºæµç¨‹ï¼‰ã€‚',
                'keywords': ['æ¶æ„', 'è®¾å¤‡', 'å·¥è‰º', 'å‚æ•°'],
                'min_length': 800
            },
            {
                'name': 'æˆæœ¬ä¼°ç®—æ¨¡å‹',
                'prompt': 'é¡¹ç›®ç±»å‹ï¼šæ±½è½¦è£…é…çº¿ï¼Œè§„æ¨¡ä¸­å‹ï¼Œè‡ªåŠ¨åŒ–ç¨‹åº¦95%ï¼Œè®¾å¤‡10å°ï¼Œå·¥æœŸ6ä¸ªæœˆã€‚è¯·æä¾›è¯¦ç»†æˆæœ¬ä¼°ç®—ã€‚',
                'keywords': ['æˆæœ¬', 'è®¾å¤‡è´¹', 'äººå·¥', 'å®æ–½'],
                'min_length': 400
            },
            {
                'name': 'æŠ¥ä»·å•ç”Ÿæˆ',
                'prompt': 'ç”Ÿæˆæ±½è½¦é›¶éƒ¨ä»¶è£…é…çº¿çš„ä¸“ä¸šæŠ¥ä»·å•ï¼Œé¢„ç®—Â¥500ä¸‡ï¼ŒåŒ…æ‹¬è®¾å¤‡æ¸…å•ã€å·¥ç¨‹æœåŠ¡ã€å”®åæœåŠ¡ã€ä»˜æ¬¾æ¡ä»¶ã€‚',
                'keywords': ['æŠ¥ä»·', 'è®¾å¤‡', 'æœåŠ¡', 'ä»˜æ¬¾'],
                'min_length': 500
            }
        ]


if __name__ == '__main__':
    comparison = ModelComparison()
    comparison.run_comparison()

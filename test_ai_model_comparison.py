#!/usr/bin/env python3
"""
AI æ¨¡å‹å¯¹æ¯”æµ‹è¯•è„šæœ¬
å¯¹æ¯” GLM-5 vs Kimi vs GPT-4 åœ¨å”®å‰AIç³»ç»Ÿä¸­çš„è¡¨ç°
"""

import os
import sys
import time
import json
from typing import Dict, List
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.services.ai_client_service import AIClientService


class Colors:
    """ç»ˆç«¯é¢œè‰²"""
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    RESET = '\033[0m'
    BOLD = '\033[1m'


class ModelComparison:
    """AIæ¨¡å‹å¯¹æ¯”æµ‹è¯•"""
    
    def __init__(self):
        # æ£€æµ‹å¯ç”¨æ¨¡å‹
        self.available_models = self.detect_available_models()
        self.test_cases = self.load_test_cases()
        self.results = []
        
        print(f"{Colors.BOLD}å¯ç”¨æ¨¡å‹: {', '.join(self.available_models)}{Colors.RESET}\n")
    
    def detect_available_models(self) -> List[str]:
        """æ£€æµ‹å“ªäº›æ¨¡å‹å¯ç”¨"""
        models = []
        
        if os.getenv("ZHIPU_API_KEY") and os.getenv("ZHIPU_API_KEY") != "your_zhipu_api_key_here":
            models.append("glm-5")
        if os.getenv("OPENAI_API_KEY"):
            models.append("gpt-4")
        if os.getenv("KIMI_API_KEY"):
            models.append("kimi")
        
        # å¦‚æœæ²¡æœ‰é…ç½®ä»»ä½•API Keyï¼Œä½¿ç”¨Mockæ¨¡å¼
        if not models:
            models = ["glm-5", "kimi", "gpt-4"]  # Mockæ¨¡å¼å…¨éƒ¨å¯ç”¨
        
        return models
    
    def run_comparison(self):
        """è¿è¡Œå®Œæ•´å¯¹æ¯”æµ‹è¯•"""
        print(f"{Colors.BOLD}{'=' * 60}{Colors.RESET}")
        print(f"{Colors.CYAN}{Colors.BOLD}AI æ¨¡å‹å¯¹æ¯”æµ‹è¯•{Colors.RESET}")
        print(f"{Colors.BOLD}{'=' * 60}{Colors.RESET}\n")
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        for case_id, test_case in enumerate(self.test_cases, 1):
            print(f"\n{Colors.BLUE}{Colors.BOLD}ğŸ“ æµ‹è¯•ç”¨ä¾‹ {case_id}: {test_case['name']}{Colors.RESET}")
            print(f"{Colors.BOLD}{'-' * 60}{Colors.RESET}\n")
            
            case_results = {
                'case_id': case_id,
                'case_name': test_case['name'],
                'models': {}
            }
            
            for model in self.available_models:
                print(f"{Colors.MAGENTA}ğŸ¤– æµ‹è¯•æ¨¡å‹: {model.upper()}{Colors.RESET}")
                result = self.test_single_model(model, test_case)
                case_results['models'][model] = result
                
                # æ‰“å°ç»“æœ
                self.print_result(model, result)
                print()  # ç©ºè¡Œåˆ†éš”
            
            self.results.append(case_results)
            
            # å¯¹æ¯”åˆ†æ
            self.compare_results(case_results)
        
        # ç”Ÿæˆæ€»ä½“æŠ¥å‘Š
        self.generate_report()
    
    def test_single_model(self, model: str, test_case: Dict) -> Dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        client = AIClientService()
        
        start_time = time.time()
        error = None
        response = None
        
        try:
            response = client.generate_solution(
                prompt=test_case['prompt'],
                model=model,
                temperature=test_case.get('temperature', 0.7),
                max_tokens=test_case.get('max_tokens', 2000)
            )
            success = True
        except Exception as e:
            error = str(e)
            success = False
        
        end_time = time.time()
        elapsed = end_time - start_time
        
        result = {
            'success': success,
            'response_time': round(elapsed, 2),
            'error': error
        }
        
        if success and response:
            result['content'] = response.get('content', '')
            result['tokens'] = response.get('usage', {}).get('total_tokens', 0)
            result['model'] = response.get('model', model)
            
            # è´¨é‡è¯„ä¼°
            result['quality_score'] = self.evaluate_quality(
                response.get('content', ''),
                test_case
            )
            
            # æˆæœ¬ä¼°ç®—
            result['estimated_cost'] = self.estimate_cost(
                model,
                result['tokens']
            )
        
        return result
    
    def evaluate_quality(self, content: str, test_case: Dict) -> float:
        """è¯„ä¼°å›å¤è´¨é‡ (1-5åˆ†)"""
        score = 3.0  # åŸºç¡€åˆ†
        
        # æ£€æŸ¥å…³é”®è¯
        keywords = test_case.get('keywords', [])
        if keywords:
            matched = sum(1 for kw in keywords if kw.lower() in content.lower())
            keyword_ratio = matched / len(keywords)
            score += keyword_ratio * 1.0
        
        # æ£€æŸ¥é•¿åº¦
        min_length = test_case.get('min_length', 500)
        if len(content) >= min_length:
            score += 0.5
        else:
            score -= 0.5
        
        # æ£€æŸ¥ç»“æ„åŒ–
        has_structure = any(marker in content for marker in ['```json', '```', '1.', '2.', 'â€¢', '-'])
        if has_structure:
            score += 0.5
        
        return min(5.0, max(1.0, round(score, 1)))
    
    def estimate_cost(self, model: str, tokens: int) -> float:
        """ä¼°ç®—æˆæœ¬ï¼ˆäººæ°‘å¸ï¼‰"""
        # ä»·æ ¼å‚è€ƒï¼ˆæ¯1K tokensï¼‰
        prices = {
            'glm-5': 0.10,      # è¾“å…¥0.05+è¾“å‡º0.15 å¹³å‡
            'kimi': 0.08,       # Kimi ä»·æ ¼
            'gpt-4': 0.30,      # GPT-4 ä»·æ ¼
        }
        
        price_per_1k = prices.get(model, 0.10)
        return round(tokens / 1000 * price_per_1k, 4)
    
    def print_result(self, model: str, result: Dict):
        """æ‰“å°å•æ¬¡æµ‹è¯•ç»“æœ"""
        if result['success']:
            print(f"  {Colors.GREEN}âœ… æˆåŠŸ{Colors.RESET}")
            print(f"  â±ï¸  å“åº”æ—¶é—´: {Colors.YELLOW}{result['response_time']}ç§’{Colors.RESET}")
            print(f"  ğŸ“Š Tokenæ¶ˆè€—: {Colors.CYAN}{result.get('tokens', 'N/A')}{Colors.RESET}")
            print(f"  â­ è´¨é‡è¯„åˆ†: {Colors.GREEN}{result.get('quality_score', 'N/A')}/5{Colors.RESET}")
            print(f"  ğŸ’° é¢„ä¼°æˆæœ¬: {Colors.YELLOW}Â¥{result.get('estimated_cost', 0)}{Colors.RESET}")
            content_preview = result.get('content', '')[:150].replace('\n', ' ')
            print(f"  ğŸ“ å†…å®¹é¢„è§ˆ: {content_preview}...")
        else:
            print(f"  {Colors.RED}âŒ å¤±è´¥: {result['error']}{Colors.RESET}")
    
    def compare_results(self, case_results: Dict):
        """å¯¹æ¯”åˆ†æå•ä¸ªç”¨ä¾‹çš„ç»“æœ"""
        print(f"\n{Colors.CYAN}{Colors.BOLD}ğŸ“Š å¯¹æ¯”åˆ†æ - {case_results['case_name']}{Colors.RESET}")
        print(f"{Colors.BOLD}{'-' * 60}{Colors.RESET}")
        
        # æ”¶é›†æ•°æ®
        models_data = []
        for model, result in case_results['models'].items():
            if result['success']:
                models_data.append({
                    'model': model,
                    'time': result['response_time'],
                    'tokens': result.get('tokens', 0),
                    'quality': result.get('quality_score', 0),
                    'cost': result.get('estimated_cost', 0)
                })
        
        if not models_data:
            print(f"  {Colors.RED}âš ï¸  æ‰€æœ‰æ¨¡å‹æµ‹è¯•å¤±è´¥{Colors.RESET}")
            return
        
        # æ‰¾å‡ºæœ€ä¼˜
        fastest = min(models_data, key=lambda x: x['time'])
        most_efficient = min(models_data, key=lambda x: x['tokens'])
        best_quality = max(models_data, key=lambda x: x['quality'])
        cheapest = min(models_data, key=lambda x: x['cost'])
        
        # è®¡ç®—æ€§ä»·æ¯”ï¼ˆè´¨é‡/æˆæœ¬ï¼‰
        for data in models_data:
            if data['cost'] > 0:
                data['value_score'] = data['quality'] / data['cost'] * 100
            else:
                data['value_score'] = data['quality'] * 100
        best_value = max(models_data, key=lambda x: x['value_score'])
        
        print(f"  {Colors.GREEN}ğŸ† æœ€å¿«: {fastest['model'].upper()} ({fastest['time']}ç§’){Colors.RESET}")
        print(f"  {Colors.CYAN}ğŸ’° æœ€çœToken: {most_efficient['model'].upper()} ({most_efficient['tokens']} tokens){Colors.RESET}")
        print(f"  {Colors.YELLOW}â­ æœ€é«˜è´¨é‡: {best_quality['model'].upper()} ({best_quality['quality']}/5){Colors.RESET}")
        print(f"  {Colors.MAGENTA}ğŸ’µ æœ€ä½æˆæœ¬: {cheapest['model'].upper()} (Â¥{cheapest['cost']}){Colors.RESET}")
        print(f"  {Colors.BOLD}ğŸ¯ æœ€ä½³æ€§ä»·æ¯”: {best_value['model'].upper()} (å¾—åˆ†: {best_value['value_score']:.1f}){Colors.RESET}")
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´å¯¹æ¯”æŠ¥å‘Š"""
        print(f"\n{Colors.BOLD}{'=' * 60}{Colors.RESET}")
        print(f"{Colors.CYAN}{Colors.BOLD}ğŸ“Š å®Œæ•´å¯¹æ¯”æŠ¥å‘Š{Colors.RESET}")
        print(f"{Colors.BOLD}{'=' * 60}{Colors.RESET}\n")
        
        # ç»Ÿè®¡æ¯ä¸ªæ¨¡å‹çš„è¡¨ç°
        model_stats = {model: {
            'success_count': 0,
            'fail_count': 0,
            'total_time': 0,
            'total_tokens': 0,
            'total_quality': 0,
            'total_cost': 0,
            'test_count': 0
        } for model in self.available_models}
        
        for case_result in self.results:
            for model, result in case_result['models'].items():
                stats = model_stats[model]
                stats['test_count'] += 1
                
                if result['success']:
                    stats['success_count'] += 1
                    stats['total_time'] += result['response_time']
                    stats['total_tokens'] += result.get('tokens', 0)
                    stats['total_quality'] += result.get('quality_score', 0)
                    stats['total_cost'] += result.get('estimated_cost', 0)
                else:
                    stats['fail_count'] += 1
        
        # æ‰“å°ç»Ÿè®¡
        comparison_table = []
        
        for model, stats in model_stats.items():
            print(f"{Colors.MAGENTA}{Colors.BOLD}ğŸ¤– {model.upper()}{Colors.RESET}")
            print(f"{Colors.BOLD}{'-' * 60}{Colors.RESET}")
            
            if stats['test_count'] == 0:
                print(f"  {Colors.RED}æœªæµ‹è¯•{Colors.RESET}\n")
                continue
            
            success_rate = (stats['success_count'] / stats['test_count'] * 100) if stats['test_count'] > 0 else 0
            avg_time = stats['total_time'] / stats['success_count'] if stats['success_count'] > 0 else 0
            avg_tokens = stats['total_tokens'] / stats['success_count'] if stats['success_count'] > 0 else 0
            avg_quality = stats['total_quality'] / stats['success_count'] if stats['success_count'] > 0 else 0
            avg_cost = stats['total_cost'] / stats['success_count'] if stats['success_count'] > 0 else 0
            
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            quality_score = (avg_quality / 5) * 100
            speed_score = (2.0 / avg_time * 100) if avg_time > 0 else 0  # å‡è®¾2ç§’æ˜¯ç†æƒ³æ—¶é—´
            cost_score = (0.10 / avg_cost * 100) if avg_cost > 0 else 100  # å‡è®¾Â¥0.10æ˜¯ç†æƒ³æˆæœ¬
            stability_score = success_rate
            
            comprehensive_score = (
                quality_score * 0.4 +
                min(speed_score, 100) * 0.3 +
                min(cost_score, 100) * 0.2 +
                stability_score * 0.1
            )
            
            print(f"  æˆåŠŸç‡: {Colors.GREEN if success_rate >= 90 else Colors.YELLOW}{success_rate:.1f}%{Colors.RESET}")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {Colors.YELLOW}{avg_time:.2f}ç§’{Colors.RESET}")
            print(f"  å¹³å‡Tokenæ¶ˆè€—: {Colors.CYAN}{avg_tokens:.0f}{Colors.RESET}")
            print(f"  å¹³å‡è´¨é‡è¯„åˆ†: {Colors.GREEN}{avg_quality:.1f}/5{Colors.RESET}")
            print(f"  å¹³å‡å•æ¬¡æˆæœ¬: {Colors.YELLOW}Â¥{avg_cost:.4f}{Colors.RESET}")
            print(f"  {Colors.BOLD}ç»¼åˆå¾—åˆ†: {Colors.GREEN if comprehensive_score >= 80 else Colors.YELLOW}{comprehensive_score:.1f}/100{Colors.RESET}\n")
            
            comparison_table.append({
                'model': model,
                'success_rate': success_rate,
                'avg_time': avg_time,
                'avg_tokens': avg_tokens,
                'avg_quality': avg_quality,
                'avg_cost': avg_cost,
                'comprehensive_score': comprehensive_score
            })
        
        # æ¨è
        if comparison_table:
            best_model = max(comparison_table, key=lambda x: x['comprehensive_score'])
            print(f"{Colors.BOLD}{'=' * 60}{Colors.RESET}")
            print(f"{Colors.GREEN}{Colors.BOLD}ğŸ† æ¨èæ¨¡å‹: {best_model['model'].upper()}{Colors.RESET}")
            print(f"{Colors.GREEN}ç»¼åˆå¾—åˆ†: {best_model['comprehensive_score']:.1f}/100{Colors.RESET}")
            print(f"{Colors.BOLD}{'=' * 60}{Colors.RESET}\n")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        report_file = f"ai_model_comparison_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'test_time': datetime.now().isoformat(),
                'test_cases': len(self.test_cases),
                'models_tested': self.available_models,
                'detailed_results': self.results,
                'summary': comparison_table,
                'recommendation': best_model if comparison_table else None
            }, f, ensure_ascii=False, indent=2)
        
        print(f"{Colors.GREEN}âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {report_file}{Colors.RESET}\n")
    
    def load_test_cases(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•ç”¨ä¾‹"""
        return [
            {
                'name': 'éœ€æ±‚ç†è§£å¼•æ“',
                'prompt': '''
å®¢æˆ·éœ€æ±‚ï¼š
æˆ‘ä»¬æ˜¯æ±½è½¦åˆ¶é€ ä¼ä¸šï¼Œéœ€è¦ä¸€å¥—è‡ªåŠ¨åŒ–è£…é…çº¿ã€‚
äº§èƒ½è¦æ±‚ï¼š100ä»¶/å°æ—¶
è‡ªåŠ¨åŒ–ç¨‹åº¦ï¼š95%ä»¥ä¸Š
éœ€è¦é›†æˆè§†è§‰æ£€æµ‹å’Œæœºå™¨äººè£…é…

è¯·åˆ†æå¹¶æå–å…³é”®éœ€æ±‚ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¿”å›ã€‚
''',
                'keywords': ['æ±½è½¦', 'è£…é…çº¿', '100', '95', 'è§†è§‰æ£€æµ‹', 'æœºå™¨äºº'],
                'min_length': 300,
                'temperature': 0.3,
                'max_tokens': 1000
            },
            {
                'name': 'æ–¹æ¡ˆç”Ÿæˆå¼•æ“',
                'prompt': '''
åŸºäºä»¥ä¸‹éœ€æ±‚ç”Ÿæˆå®Œæ•´æŠ€æœ¯æ–¹æ¡ˆï¼š
- è¡Œä¸š: æ±½è½¦åˆ¶é€ 
- äº§èƒ½: 100ä»¶/å°æ—¶
- è‡ªåŠ¨åŒ–ç¨‹åº¦: 95%
- å…³é”®æŠ€æœ¯: è§†è§‰æ£€æµ‹ã€æœºå™¨äººè£…é…

è¯·åŒ…å«ï¼šç³»ç»Ÿæ¶æ„ã€è®¾å¤‡æ¸…å•ã€å·¥è‰ºæµç¨‹ã€æŠ€æœ¯å‚æ•°
''',
                'keywords': ['æ¶æ„', 'è®¾å¤‡', 'å·¥è‰º', 'å‚æ•°', 'æœºå™¨äºº', 'è§†è§‰'],
                'min_length': 800,
                'temperature': 0.7,
                'max_tokens': 2000
            },
            {
                'name': 'æˆæœ¬ä¼°ç®—æ¨¡å‹',
                'prompt': '''
é¢„ä¼°é¡¹ç›®æˆæœ¬ï¼š
- é¡¹ç›®ç±»å‹: æ±½è½¦è£…é…çº¿
- è§„æ¨¡: ä¸­å‹
- è‡ªåŠ¨åŒ–ç¨‹åº¦: 95%
- è®¾å¤‡æ•°é‡: 10å°
- å·¥æœŸ: 6ä¸ªæœˆ

è¯·æä¾›è¯¦ç»†æˆæœ¬ä¼°ç®—ï¼ˆè®¾å¤‡ã€äººå·¥ã€å®æ–½ã€è°ƒè¯•ç­‰ï¼‰
''',
                'keywords': ['æˆæœ¬', 'è®¾å¤‡è´¹', 'äººå·¥', 'å®æ–½', 'è°ƒè¯•', 'æ€»è®¡'],
                'min_length': 400,
                'temperature': 0.5,
                'max_tokens': 1500
            },
            {
                'name': 'æŠ¥ä»·å•ç”Ÿæˆ',
                'prompt': '''
ç”Ÿæˆä¸“ä¸šæŠ¥ä»·å•ï¼š
- é¡¹ç›®: æ±½è½¦é›¶éƒ¨ä»¶è£…é…çº¿
- é¢„ç®—èŒƒå›´: Â¥500ä¸‡

åŒ…æ‹¬ï¼šè®¾å¤‡æ¸…å•ã€å·¥ç¨‹æœåŠ¡ã€å”®åæœåŠ¡ã€ä»˜æ¬¾æ¡ä»¶
''',
                'keywords': ['æŠ¥ä»·', 'è®¾å¤‡', 'æœåŠ¡', 'ä»˜æ¬¾', 'æ€»è®¡'],
                'min_length': 500,
                'temperature': 0.5,
                'max_tokens': 1500
            }
        ]


def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{Colors.CYAN}{Colors.BOLD}{'=' * 60}")
    print("AI æ¨¡å‹å¯¹æ¯”æµ‹è¯•å·¥å…·")
    print(f"{'=' * 60}{Colors.RESET}\n")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    print(f"{Colors.YELLOW}ğŸ“Œ ç¯å¢ƒæ£€æŸ¥:{Colors.RESET}")
    print(f"  ZHIPU_API_KEY: {'âœ… å·²é…ç½®' if os.getenv('ZHIPU_API_KEY') and os.getenv('ZHIPU_API_KEY') != 'your_zhipu_api_key_here' else 'âŒ æœªé…ç½®'}")
    print(f"  OPENAI_API_KEY: {'âœ… å·²é…ç½®' if os.getenv('OPENAI_API_KEY') else 'âŒ æœªé…ç½®'}")
    print(f"  KIMI_API_KEY: {'âœ… å·²é…ç½®' if os.getenv('KIMI_API_KEY') else 'âŒ æœªé…ç½®'}")
    
    if not any([
        os.getenv('ZHIPU_API_KEY') and os.getenv('ZHIPU_API_KEY') != 'your_zhipu_api_key_here',
        os.getenv('OPENAI_API_KEY'),
        os.getenv('KIMI_API_KEY')
    ]):
        print(f"\n{Colors.YELLOW}âš ï¸  æœªé…ç½®ä»»ä½•API Keyï¼Œå°†ä½¿ç”¨Mockæ¨¡å¼æµ‹è¯•{Colors.RESET}")
    
    print()
    
    # è¿è¡Œå¯¹æ¯”æµ‹è¯•
    comparison = ModelComparison()
    comparison.run_comparison()
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
